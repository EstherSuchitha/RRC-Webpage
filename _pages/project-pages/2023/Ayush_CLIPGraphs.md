---
layout: project-page-new
title: "CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities"
authors:
  - name: Ayush Agrawal∗
    sup: 1
  - name: Raghav Arora∗
    sup: 1
  - name: Ahana Datta
    sup: 1
  - name: Snehasis Banerjee
    sup: 1
  - name: Brojeshwar Bhowmick
    sup: 1
  - name: Krishna Murthy Jatavallabhula
    sup: 1
  - name: Mohan Sridharan
    sup: 1
  - name: Madhava Krishna
    sup: 1
affiliations:
  - name: IIIT Hyderabad, India
    link: https://robotics.iiit.ac.in
    sup: 1
permalink: /publications/2023/Ayush_CLIPGraphs/
abstract: "This paper introduces a novel method for determining the best room to place an object in, for embodied scene rearrangement. While state-of-the-art approaches rely on large language models (LLMs) or reinforcement learned (RL) policies for this task, our approach, CLIPGraphs, efficiently combines commonsense domain knowledge, data-driven methods, and recent advances in multimodal learning. Specifically, it (a) encodes a knowledge graph of prior human preferences about the room location of different objects in home environments,
(b) incorporates vision-language features to support multimodal
queries based on images or text, and (c) uses a graph network to
learn object-room affinities based on embeddings of the prior
knowledge and the vision-language features. We demonstrate that our approach provides better estimates of the most appropriate location of objects from a benchmark set of object categories in comparison with state-of-the-art baselines."
project_page: https://clipgraphs.github.io/
paper: https://arxiv.org/pdf/2306.01540.pdf
code: https://github.com/CLIPGraphs/CLIPGraphs.github.io/tree/irona
supplement: https://clipgraphs.github.io/static/pdfs/Supplementary.pdf
#video: https://robotics.iiit.ac.in/publications/2020/deep-mpc-for-visual-servoing/video.mp4
iframe: https://www.youtube.com/embed/aZgyFGuWZbk

---