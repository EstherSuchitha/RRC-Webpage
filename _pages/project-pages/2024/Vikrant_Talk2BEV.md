---
layout: project-page-new
title: "Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving"
authors:
  - name: Vikrant Dewangan∗
    sup: 1
  - name: Tushar Choudhary∗
    sup: 1
  - name: Shivam Chandhok∗
    sup: 2
  - name: Shubham Priyadarshan
    sup: 1
  - name: Anushka Jain
    sup: 1
  - name: Arun K. Singh
    sup: 3
  - name: Siddharth Srivastava
    sup: 4
  - name: Krishna Murthy Jatavallabhula†
    sup: 5
  - name: K. Madhava Krishna†
    sup: 1
affiliations:
  - name: Robotics Research Center, IIIT Hyderabad, India
    link: https://robotics.iiit.ac.in
    sup: 1
  - name: University of British Columbia
    link: https://www.ubc.ca/
    sup: 2
  - name: University of Tartu
    link: https://ut.ee/en/home
    sup: 3
  - name: TensorTour Inc
    link: https://www.typeface.ai/
    sup: 4
  - name: MIT Computer Science & Artificial Intelligence Laboratory
    link: https://www.csail.mit.edu/
    sup: 5
permalink: /publications/2024/Vikrant_Talk2BEV/
<<<<<<< HEAD
abstract: "This work introduces Talk2BEV, a large vision-language model(LVLM)1 interface for bird’s-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representations, eliminating the need for task-specific models. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret free-form natural language queries, and in grounding these queries to the visual context embedded into the language- enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset."
=======
abstract: "This work introduces Talk2BEV, a large vision-language model (LVLM)1 interface for bird’s-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representations, eliminating the need for task-specific models. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret free-form natural language queries, and in grounding these queries to the visual context embedded into the language- enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset."
>>>>>>> 9b7f223d1a33a7ee06bc77ea0b0dddaaf9d596da
project_page: https://llmbev.github.io/talk2bev/
paper: https://arxiv.org/pdf/2310.02251
code: https://github.com/llmbev/talk2bev
#supplement: https://clipgraphs.github.io/static/pdfs/Supplementary.pdf
#video: https://www.youtube.com/watch?v=TMht-8SGJ0I
iframe: https://www.youtube.com/embed/TMht-8SGJ0I
#demo: https://anyloc.github.io/#interactive_demo

---